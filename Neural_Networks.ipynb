{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e817f930c2ad41ceb6e14dca011f5b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d08e6a51edc54fc4b0d5c570f83fb29d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23907b5f67154b09ab4e32b514be993a",
              "IPY_MODEL_637135540efb477193462ae6681847f1",
              "IPY_MODEL_a8f63f0afc4046579ed922bd3edd6f52"
            ]
          }
        },
        "d08e6a51edc54fc4b0d5c570f83fb29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23907b5f67154b09ab4e32b514be993a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_381a9c0fb7b3434a8c653e12d1d97a21",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  7%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_038d0f9c858f457a9de13cdb92177424"
          }
        },
        "637135540efb477193462ae6681847f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9431b02b157041aa92167c43534ae202",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99bf696e2a9f4434b6f8fc5f9b44a7b0"
          }
        },
        "a8f63f0afc4046579ed922bd3edd6f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66c932f9db864e518d060401ca1bb084",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14/200 [01:53&lt;25:01,  8.07s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dc2ac1c7e0f48ddabaee767f63fe71a"
          }
        },
        "381a9c0fb7b3434a8c653e12d1d97a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "038d0f9c858f457a9de13cdb92177424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9431b02b157041aa92167c43534ae202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99bf696e2a9f4434b6f8fc5f9b44a7b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66c932f9db864e518d060401ca1bb084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dc2ac1c7e0f48ddabaee767f63fe71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0z4lw515O6i",
        "outputId": "870cfe89-f785-493c-b326-47b35dd5e8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxla2Rq0QYNz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import CrossEntropyLoss, ReLU,DataParallel\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from scipy import stats\n",
        "import logging\n",
        "from _datetime import datetime as dt0\n",
        "import math\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "jw1CEuVjQ69Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassClassification(nn.Module):\n",
        "    def __init__(self, num_feature, num_class):\n",
        "        super(MulticlassClassification, self).__init__()\n",
        "\n",
        "        self.layer_1 = nn.Linear(num_feature, 64)\n",
        "        self.layer_2 = nn.Linear(64, 16)\n",
        "        self.layer_out = nn.Linear(16, num_class) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(16)\n",
        "\n",
        "        # self.layer_1 = nn.Linear(num_feature,32)\n",
        "        # self.layer_out = nn.Linear(32, num_class)\n",
        "\n",
        "        # self.batchnorm1 = nn.BatchNorm1d(32)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = gelu(x)\n",
        "        # x = self.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = gelu(x)\n",
        "        # x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "\n",
        "        # x = self.layer_1(x)\n",
        "        # x = self.batchnorm1(x)\n",
        "        # x = gelu(x)\n",
        "\n",
        "        # x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "vgrh5QBiQwXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_labels(label):\n",
        "    if label == \"O\":\n",
        "        return 0\n",
        "    elif label==\"B-NEG\":\n",
        "        return 1\n",
        "    elif label == \"I-NEG\":\n",
        "        return 2\n",
        "    else:\n",
        "        print(label)\n",
        "        print(\"Error\")\n",
        "        return 2\n",
        "\n",
        "def load_train_val_data():\n",
        "\n",
        "    train_df = pd.read_csv('trainset_final.csv', sep = \";\")\n",
        "    dev_df = pd.read_csv('devset_final.csv', sep = \";\")   \n",
        "\n",
        "    # train_df = train_df.loc[ train_df[\"id\"].str.contains(\"baskervilles\")]\n",
        "\n",
        "    train_df[\"Label\"] = train_df[\"Label\"].apply(lambda x: process_labels(x))\n",
        "    dev_df[\"Label\"] = dev_df[\"Label\"].apply(lambda x: process_labels(x))\n",
        "\n",
        "    train_labels = train_df[\"Label\"]\n",
        "    dev_labels = dev_df[\"Label\"]\n",
        "\n",
        "    train_df.drop([\"Label\", \"id\"], axis =1, inplace = True)\n",
        "    dev_df.drop([\"Label\",\"id\"], axis =1, inplace = True)\n",
        "\n",
        "    missing_features_dev = [col for col in train_df.columns if col not in dev_df]\n",
        "\n",
        "    dev_df[missing_features_dev] = 0\n",
        "\n",
        "    substring_list = [\"Lemma\", \"Grammar_phrase\", \"Has\", \"POS_\", \"next_1\", \"previous_1\", \"next_2\", \"previous_2\", \"Is_\", \"Named_Entity\"]\n",
        "\n",
        "    columns_to_keep = [col for col in train_df.columns if any(substring in col for substring in substring_list)]\n",
        "\n",
        "    train_df = train_df[columns_to_keep]\n",
        "    dev_df = dev_df[columns_to_keep]\n",
        "\n",
        "    print( len(train_df.columns), len(dev_df.columns))\n",
        "\n",
        "    # train_df = train_df[['Lemma_embedding_1', 'Lemma_embedding_2', 'Lemma_embedding_3', 'Lemma_embedding_4', 'Lemma_embedding_5', 'Label']]\n",
        "    # dev_df = dev_df[['Lemma_embedding_1', 'Lemma_embedding_2', 'Lemma_embedding_3', 'Lemma_embedding_4', 'Lemma_embedding_5', 'Label']]\n",
        "\n",
        "    return train_df, dev_df, train_labels, dev_labels\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5zR4yICRtdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n"
      ],
      "metadata": {
        "id": "BvhGH1j2a6Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_class_weights(array):\n",
        "    counts = [0,0,0]\n",
        "\n",
        "    for item in array:\n",
        "        if item == 0:\n",
        "            counts[0] +=1\n",
        "        elif item == 1:\n",
        "            counts[1] +=1\n",
        "        else:\n",
        "            counts[2] +=1\n",
        "\n",
        "    return np.asarray( counts )"
      ],
      "metadata": {
        "id": "LT4EVloNbfyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, dev_df, train_labels, dev_labels = load_train_val_data()\n",
        "X_train = train_df.to_numpy()\n",
        "y_train = train_labels.to_numpy().astype('int64')\n",
        "X_dev = dev_df.to_numpy()\n",
        "y_dev = dev_labels.to_numpy().astype('int64')\n",
        "\n",
        "train_dataset = ClassifierDataset(torch.from_numpy( X_train ).float(), torch.from_numpy(y_train).long())\n",
        "val_dataset = ClassifierDataset(torch.from_numpy(X_dev ).float(), torch.from_numpy(y_dev).long())\n",
        "\n",
        "#For the class imbalance\n",
        "target_list = []\n",
        "for _, t in train_dataset:\n",
        "    target_list.append(t)\n",
        "target_list = torch.tensor(target_list)\n",
        "\n",
        "class_count = return_class_weights(y_train)\n",
        "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
        "class_weights_all = class_weights[target_list]\n",
        "\n",
        "\n",
        "for col in train_df.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly6MKkEgbCnh",
        "outputId": "1d229152-1a12-450e-fd52-3ba5e43e4532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 81\n",
            "Is_popular_token\n",
            "Is_begin_token\n",
            "Is_end_token\n",
            "Is_early_token\n",
            "Has_possible_prefix\n",
            "Has_subfix\n",
            "Has_correlating_synonym\n",
            "Has_correct_antonym\n",
            "Has_antonyms\n",
            "Grammar_phrase_CLAUSE\n",
            "Grammar_phrase_CLAUSE_2\n",
            "Grammar_phrase_NP_2\n",
            "Grammar_phrase_PP\n",
            "Grammar_phrase_PP_2\n",
            "Grammar_phrase_VP\n",
            "Grammar_phrase_VP_2\n",
            "POS_CD\n",
            "POS_DT\n",
            "POS_EX\n",
            "POS_FW\n",
            "POS_IN\n",
            "POS_JJ\n",
            "POS_JJR\n",
            "POS_JJS\n",
            "POS_MD\n",
            "POS_NN\n",
            "POS_NNP\n",
            "POS_NNPS\n",
            "POS_NNS\n",
            "POS_PDT\n",
            "POS_PRP\n",
            "POS_PRP$\n",
            "POS_RB\n",
            "POS_RBR\n",
            "POS_RBS\n",
            "POS_RP\n",
            "POS_TO\n",
            "POS_UH\n",
            "POS_VB\n",
            "POS_VBD\n",
            "POS_VBG\n",
            "POS_VBN\n",
            "POS_VBP\n",
            "POS_VBZ\n",
            "POS_WDT\n",
            "POS_WP\n",
            "POS_WP$\n",
            "POS_WRB\n",
            "Named_Entity_GPE\n",
            "Named_Entity_GSP\n",
            "Named_Entity_LOCATION\n",
            "Named_Entity_ORGANIZATION\n",
            "Named_Entity_PERSON\n",
            "POS_CC\n",
            "POS_LS\n",
            "POS_SYM\n",
            "Lemma_embedding_1\n",
            "Lemma_embedding_2\n",
            "Lemma_embedding_3\n",
            "Lemma_embedding_4\n",
            "Lemma_embedding_5\n",
            "next_1_0\n",
            "next_1_1\n",
            "next_1_2\n",
            "next_1_3\n",
            "next_1_4\n",
            "next_2_0\n",
            "next_2_1\n",
            "next_2_2\n",
            "next_2_3\n",
            "next_2_4\n",
            "previous_1_0\n",
            "previous_1_1\n",
            "previous_1_2\n",
            "previous_1_3\n",
            "previous_1_4\n",
            "previous_2_0\n",
            "previous_2_1\n",
            "previous_2_2\n",
            "previous_2_3\n",
            "previous_2_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_sampler = WeightedRandomSampler(\n",
        "    weights=class_weights_all,\n",
        "    num_samples=len(class_weights_all),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=32,\n",
        "                    sampler= weighted_sampler\n",
        ")\n",
        "val_loader = DataLoader(dataset=val_dataset, \n",
        "                    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "id": "G2cFKXLPV3yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hidden layer size based on formula here : https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=The%20number%20of%20hidden%20neurons,size%20of%20the%20input%20layer.\n",
        "# alpha I chose here is 10, but that still gave me a large upperbound, so chose 50 for now\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MulticlassClassification(len(train_df.columns), 3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #\n",
        "\n",
        "# Best known optimiser: AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=0.0005,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "batch_size =16\n",
        "\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=total_steps / 5,  # Default value in run_glue.py\n",
        "                                            num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "sb4yqxywU53f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "id": "YmDbhgskV2lP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60b02d0-cee6-4b2d-fb4d-959393949af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
        "    \n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    \n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "accuracy_stats = {\n",
        "    'train': [],\n",
        "    \"val\": []\n",
        "}\n",
        "\n",
        "loss_stats = {\n",
        "    'train': [],\n",
        "    \"val\": []\n",
        "}\n"
      ],
      "metadata": {
        "id": "kdX_YccWgL8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for e in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    # TRAINING\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    train_predictions = []\n",
        "    val_predictions = []\n",
        "\n",
        "    true_train_values = []\n",
        "    true_val_values = []\n",
        "\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        y_train_pred = model(X_train_batch)\n",
        "\n",
        "        train_loss = criterion(y_train_pred, y_train_batch)\n",
        "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_epoch_loss += train_loss.item()\n",
        "        train_epoch_acc += train_acc.item()\n",
        "\n",
        "        y_pred = np.asarray( y_train_pred.argmax(1).cpu().detach() ,dtype = int )\n",
        "        y_batch = np.asarray( y_train_batch.cpu().detach(), dtype = int)\n",
        "        \n",
        "        train_predictions.extend( y_pred )\n",
        "        true_train_values.extend( y_batch )\n",
        "    \n",
        "    # VALIDATION    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        val_epoch_loss = 0\n",
        "        val_epoch_acc = 0\n",
        "        \n",
        "        model.eval()\n",
        "        for X_val_batch, y_val_batch in val_loader:\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "            \n",
        "            y_val_pred = model(X_val_batch)\n",
        "\n",
        "            val_loss = criterion(y_val_pred, y_val_batch)\n",
        "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
        "            \n",
        "            val_epoch_loss += val_loss.item()\n",
        "            val_epoch_acc += val_acc.item()\n",
        "\n",
        "            y_pred = np.asarray( y_val_pred.argmax(1).cpu().detach(), dtype = int)\n",
        "            y_batch = np.asarray( y_val_batch.cpu().detach(), dtype = int )\n",
        "            \n",
        "            val_predictions.extend( y_pred )\n",
        "            true_val_values.extend( y_batch )\n",
        "\n",
        "    # train_predictions = [ list(x.cpu().detach().numpy()) for x in train_predictions]\n",
        "    # train_predictions = [x.index(max(x)) for x in train_predictions]\n",
        "    # true_train_values = [x.cpu().detach().numpy() for x in true_train_values]\n",
        "\n",
        "    # val_predictions = [list(x.cpu().detach().numpy()) for x in val_predictions]\n",
        "    # val_predictions = [x.index(max(x)) for x in val_predictions]\n",
        "    # true_val_values = [x.cpu().detach().numpy() for x in true_val_values]\n",
        "    \n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
        "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
        "                            \n",
        "    print(\"---------------------------------------------\")\n",
        "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}\\\n",
        "    | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
        "\n",
        "    print(\"Training F1: \", f1_score(y_true = true_train_values, y_pred = train_predictions, average='macro') )\n",
        "    print(\"Validation F1: \", f1_score(y_true = true_val_values, y_pred = val_predictions, average='macro') )\n",
        "\n",
        "    print(\"---------------------------------------------\")"
      ],
      "metadata": {
        "id": "pqsdQEZKgfLL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e817f930c2ad41ceb6e14dca011f5b09",
            "d08e6a51edc54fc4b0d5c570f83fb29d",
            "23907b5f67154b09ab4e32b514be993a",
            "637135540efb477193462ae6681847f1",
            "a8f63f0afc4046579ed922bd3edd6f52",
            "381a9c0fb7b3434a8c653e12d1d97a21",
            "038d0f9c858f457a9de13cdb92177424",
            "9431b02b157041aa92167c43534ae202",
            "99bf696e2a9f4434b6f8fc5f9b44a7b0",
            "66c932f9db864e518d060401ca1bb084",
            "2dc2ac1c7e0f48ddabaee767f63fe71a"
          ]
        },
        "outputId": "23928279-0bd9-46af-8143-3885a3018b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e817f930c2ad41ceb6e14dca011f5b09",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "Epoch 001: | Train Loss: 1.08238 | Val Loss: 0.82463    | Train Acc: 40.127| Val Acc: 84.958\n",
            "Training F1:  0.3957856378566739\n",
            "Validation F1:  0.3342766264500983\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 002: | Train Loss: 0.77274 | Val Loss: 0.72369    | Train Acc: 72.367| Val Acc: 68.185\n",
            "Training F1:  0.7223847789319239\n",
            "Validation F1:  0.2933526391917484\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 003: | Train Loss: 0.48596 | Val Loss: 0.72070    | Train Acc: 86.566| Val Acc: 62.664\n",
            "Training F1:  0.8636879778955638\n",
            "Validation F1:  0.2791578061368848\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 004: | Train Loss: 0.30179 | Val Loss: 0.77637    | Train Acc: 91.177| Val Acc: 64.190\n",
            "Training F1:  0.909385271360743\n",
            "Validation F1:  0.28344166610803256\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 005: | Train Loss: 0.21263 | Val Loss: 1.12222    | Train Acc: 93.229| Val Acc: 55.767\n",
            "Training F1:  0.9299054935652618\n",
            "Validation F1:  0.2553786889403556\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 006: | Train Loss: 0.16555 | Val Loss: 1.06982    | Train Acc: 94.463| Val Acc: 62.780\n",
            "Training F1:  0.9425068656231733\n",
            "Validation F1:  0.27532703484682575\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 007: | Train Loss: 0.13671 | Val Loss: 1.65485    | Train Acc: 95.276| Val Acc: 52.151\n",
            "Training F1:  0.9510220141925152\n",
            "Validation F1:  0.24391349824050282\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 008: | Train Loss: 0.11772 | Val Loss: 1.78470    | Train Acc: 95.956| Val Acc: 54.387\n",
            "Training F1:  0.9580940301689913\n",
            "Validation F1:  0.2501841370340973\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 009: | Train Loss: 0.10697 | Val Loss: 1.97035    | Train Acc: 96.331| Val Acc: 56.271\n",
            "Training F1:  0.9617940706082858\n",
            "Validation F1:  0.2559857988307499\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 010: | Train Loss: 0.09437 | Val Loss: 1.86824    | Train Acc: 96.697| Val Acc: 57.474\n",
            "Training F1:  0.9655852507185605\n",
            "Validation F1:  0.2604150314778469\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 011: | Train Loss: 0.08958 | Val Loss: 2.43193    | Train Acc: 96.994| Val Acc: 58.716\n",
            "Training F1:  0.9687502064065869\n",
            "Validation F1:  0.2631216628860838\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 012: | Train Loss: 0.08333 | Val Loss: 2.40828    | Train Acc: 97.200| Val Acc: 57.196\n",
            "Training F1:  0.9709888690836691\n",
            "Validation F1:  0.2575940345872129\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 013: | Train Loss: 0.07674 | Val Loss: 2.29674    | Train Acc: 97.405| Val Acc: 56.736\n",
            "Training F1:  0.9729984271246915\n",
            "Validation F1:  0.2583317600091842\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Epoch 014: | Train Loss: 0.07188 | Val Loss: 2.51846    | Train Acc: 97.610| Val Acc: 53.749\n",
            "Training F1:  0.9750865927101412\n",
            "Validation F1:  0.24895543549887356\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-338-c012f56df8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_data(train_df):\n",
        "    test_df = pd.read_csv('testset_final.csv', sep = \";\")   \n",
        "\n",
        "    test_df[\"Label\"] = test_df[\"Label\"].apply(lambda x: process_labels(x))\n",
        "\n",
        "    test_labels = test_df[\"Label\"]\n",
        "\n",
        "    test_df.drop([\"Label\", \"id\"], axis =1, inplace = True)\n",
        "\n",
        "    available_features_test = [col for col in test_df.columns if col in train_df.columns]\n",
        "    missing_features_test = [col for col in train_df.columns if col not in available_features_test]\n",
        "\n",
        "    test_df = test_df[available_features_test]\n",
        "    test_df[missing_features_test] = 0\n",
        "\n",
        "    print( len(test_df.columns), len(train_df.columns) )\n",
        "\n",
        "    return test_df, test_labels\n",
        "\n",
        "test_df, test_labels = load_test_data(train_df)\n"
      ],
      "metadata": {
        "id": "pG6dok1GDJMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.to_numpy()\n",
        "y_test = test_labels.to_numpy().astype('int64')\n",
        "\n",
        "test_dataset = ClassifierDataset(torch.from_numpy( X_test ).float(), torch.from_numpy(y_test).long())\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                    batch_size=16\n",
        ")\n",
        "\n",
        "test_predictions = []"
      ],
      "metadata": {
        "id": "ITXjPHxjEu-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    model.eval()\n",
        "    for X_test_batch, y_test_batch in test_loader:\n",
        "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "        \n",
        "        y_test_pred = model(X_test_batch)\n",
        "\n",
        "        y_pred = np.asarray( y_test_pred.argmax(1).cpu().detach(), dtype = int)\n",
        "        test_predictions.extend( y_pred )\n"
      ],
      "metadata": {
        "id": "pDNgKYPZEjAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clsf_report = pd.DataFrame(classification_report(y_true = test_labels, y_pred = test_predictions, output_dict = True)).transpose()\n",
        "clsf_report"
      ],
      "metadata": {
        "id": "iBuNPyr_Eiss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def revert_labels(label):\n",
        "    if label == 0:\n",
        "        return \"O\"\n",
        "    elif label==1:\n",
        "        return \"B-NEG\"\n",
        "    elif label == 2:\n",
        "        return \"I-NEG\"\n",
        "    else:\n",
        "        print(label)\n",
        "        print(\"Error\")\n",
        "        return 0\n",
        "\n",
        "conf_df = pd.DataFrame(list(zip(test_labels, test_predictions)), columns = [\"Label\", \"Predictions\"])\n",
        "\n",
        "conf_df[\"Label\"] = conf_df[\"Label\"].apply(lambda x: revert_labels(x))\n",
        "conf_df[\"Predictions\"] = conf_df[\"Predictions\"].apply(lambda x: revert_labels(x))\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(conf_df['Label'], conf_df['Predictions'], rownames=['Actual'], colnames = ['Predicted'])\n",
        "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.suptitle('Neural Networks')\n",
        "#plt.show()\n",
        "plt.savefig('Neural Networks.pdf')"
      ],
      "metadata": {
        "id": "gskfKd0BtCw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install skorch"
      ],
      "metadata": {
        "id": "1qge6FU2_56J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from skorch import NeuralNetClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# net = NeuralNetClassifier(model\n",
        "#                          , max_epochs=100\n",
        "#                          , lr=0.001\n",
        "#                          , verbose=1\n",
        "#                          , criterion = criterion)\n",
        "\n",
        "# params = {\n",
        "#     'lr': [0.001,0.005, 0.01, 0.05],\n",
        "#     'max_epochs': list(range(10,100, 20))}\n",
        "\n",
        "# gs = GridSearchCV(net, params, refit=False, scoring='f1_micro', verbose=1, cv=5)\n",
        "# y_train = y_train.reshape(-1, 1).squeeze()\n",
        "# gs.fit(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())"
      ],
      "metadata": {
        "id": "gx_MbJyK8qRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(gs.best_params_)\n",
        "# lr = 0.01\n",
        "# epochs = 90"
      ],
      "metadata": {
        "id": "0h8NzwnrjQlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "# classifier_NN = gs.best_estimator_\n",
        "# y_dev = y_dev.reshape(-1, 1).squeeze()\n",
        "# predictions_NN = classifier_NN.predict(torch.from_numpy(X_dev).float())\n",
        "# clsf_report = pd.DataFrame(classification_report(y_true=y_dev, y_pred=predictions_NN, output_dict = True)).transpose()\n",
        "# clsf_report"
      ],
      "metadata": {
        "id": "MB6P4t4tjSkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}