{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNCW1dSMrR-6",
        "outputId": "bcde4213-a87b-4c26-e278-07b9862807c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn .svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "#LGBM:\n",
        "import lightgbm as lgbm\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3cV2gnZsorB"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkay3rPGp5vP"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk0nU0bZ_KEb",
        "outputId": "1b28feab-d708-41a0-80be-6a37793e146f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having all features that could be informative."
      ],
      "metadata": {
        "id": "ZbC-PWdQRqdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmV5gt0_qLS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "e20d92b5-81f2-43a7-bea9-7d777755f746"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-34767f0daa63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m df_train_preprocess = pd.read_csv(\n\u001b[0;32m----> 3\u001b[0;31m     'trainset_final (1).csv', sep=';')#,header=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainset_final (1).csv'"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "df_train_preprocess = pd.read_csv(\n",
        "    'trainset_final (1).csv', sep=';')#,header=True)\n",
        "\n",
        "# Dev\n",
        "df_dev_preprocess = pd.read_csv(\n",
        "    'devset_final.csv', sep=';')#,header=True)\n",
        "\n",
        "# Test\n",
        "df_test_preprocess = pd.read_csv(\n",
        "    'testset_final.csv', sep=';')\n",
        "\n",
        "df_train = df_train_preprocess\n",
        "df_dev = df_dev_preprocess\n",
        "df_test = df_test_preprocess\n",
        "\n",
        "df_train['POS'] = df_train[['POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB']].idxmax(axis=1).astype(dtype=\"category\")\n",
        "df_train['NE'] = df_train[['Named_Entity_GPE', 'Named_Entity_LOCATION', 'Named_Entity_ORGANIZATION', 'Named_Entity_PERSON']].idxmax(axis=1).astype(dtype=\"category\")\n",
        "df_train = df_train.drop(['Named_Entity_GPE', 'Named_Entity_LOCATION', 'Named_Entity_ORGANIZATION', 'Named_Entity_PERSON', 'id', 'Sentence', 'Word_N', 'Token_N', 'Named_Entity_GSP', 'POS_FW', 'POS_NN', 'POS_CC', 'POS_LS', 'POS_SYM', 'POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB'], axis =1)\n",
        "\n",
        "df_dev['POS'] = df_dev[['POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB']].idxmax(axis=1).astype(dtype=\"category\")\n",
        "df_dev = df_dev.drop(['id', 'Sentence', 'Word_N', 'Token_N', 'Named_Entity_GSP','POS_FW', 'POS_NN','POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB'], axis =1)\n",
        "\n",
        "df_test['POS'] = df_test[['POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB']].idxmax(axis=1).astype(dtype=\"category\")\n",
        "df_test['NE'] = df_test[['Named_Entity_GPE', 'Named_Entity_LOCATION', 'Named_Entity_ORGANIZATION', 'Named_Entity_PERSON']].idxmax(axis=1).astype(dtype=\"category\")\n",
        "df_test = df_test.drop(['Named_Entity_GPE', 'Named_Entity_LOCATION', 'Named_Entity_ORGANIZATION', 'Named_Entity_PERSON','id', 'Sentence', 'Word_N', 'Token_N', 'POS_NN', 'POS_CD', 'POS_DT', 'POS_EX', 'POS_IN', 'POS_JJ', 'POS_JJR', 'POS_JJS', 'POS_MD', 'POS_NNP', 'POS_NNPS', 'POS_NNS', 'POS_PDT', 'POS_PRP', 'POS_PRP$', 'POS_RB', 'POS_RBR', 'POS_RBS', 'POS_RP', 'POS_TO', 'POS_UH', 'POS_VB', 'POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ', 'POS_WDT', 'POS_WP', 'POS_WP$', 'POS_WRB'], axis =1)\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)\n",
        "print(df_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove embedding features"
      ],
      "metadata": {
        "id": "JWSso-_TRv7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T_d5E6bnHUH"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(['Lemma_embedding_1', 'Lemma_embedding_2',\n",
        "       'Lemma_embedding_3', 'Lemma_embedding_4', 'Lemma_embedding_5',\n",
        "       'next_1_0', 'next_1_1', 'next_1_2', 'next_1_3', 'next_1_4', 'next_2_0',\n",
        "       'next_2_1', 'next_2_2', 'next_2_3', 'next_2_4', 'next_3_0', 'next_3_1',\n",
        "       'next_3_2', 'next_3_3', 'next_3_4', 'previous_1_0', 'previous_1_1',\n",
        "       'previous_1_2', 'previous_1_3', 'previous_1_4', 'previous_2_0',\n",
        "       'previous_2_1', 'previous_2_2', 'previous_2_3', 'previous_2_4',\n",
        "       'previous_3_0', 'previous_3_1', 'previous_3_2', 'previous_3_3',\n",
        "       'previous_3_4'], axis=1)\n",
        "\n",
        "df_dev = df_dev.drop(['Lemma_embedding_1', 'Lemma_embedding_2',\n",
        "       'Lemma_embedding_3', 'Lemma_embedding_4', 'Lemma_embedding_5',\n",
        "       'next_1_0', 'next_1_1', 'next_1_2', 'next_1_3', 'next_1_4', 'next_2_0',\n",
        "       'next_2_1', 'next_2_2', 'next_2_3', 'next_2_4', 'next_3_0', 'next_3_1',\n",
        "       'next_3_2', 'next_3_3', 'next_3_4', 'previous_1_0', 'previous_1_1',\n",
        "       'previous_1_2', 'previous_1_3', 'previous_1_4', 'previous_2_0',\n",
        "       'previous_2_1', 'previous_2_2', 'previous_2_3', 'previous_2_4',\n",
        "       'previous_3_0', 'previous_3_1', 'previous_3_2', 'previous_3_3',\n",
        "       'previous_3_4'], axis=1)\n",
        "\n",
        "df_test = df_test.drop(['Lemma_embedding_1', 'Lemma_embedding_2',\n",
        "       'Lemma_embedding_3', 'Lemma_embedding_4', 'Lemma_embedding_5',\n",
        "       'next_1_0', 'next_1_1', 'next_1_2', 'next_1_3', 'next_1_4', 'next_2_0',\n",
        "       'next_2_1', 'next_2_2', 'next_2_3', 'next_2_4', 'next_3_0', 'next_3_1',\n",
        "       'next_3_2', 'next_3_3', 'next_3_4', 'previous_1_0', 'previous_1_1',\n",
        "       'previous_1_2', 'previous_1_3', 'previous_1_4', 'previous_2_0',\n",
        "       'previous_2_1', 'previous_2_2', 'previous_2_3', 'previous_2_4',\n",
        "       'previous_3_0', 'previous_3_1', 'previous_3_2', 'previous_3_3',\n",
        "       'previous_3_4'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop min_token_span and has_correct_antonym"
      ],
      "metadata": {
        "id": "AjhXtl2YSajw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['Min_token_span', 'Has_correct_antonym'], axis=1)\n",
        "df_dev = df_dev.drop(['Min_token_span', 'Has_correct_antonym'], axis=1)\n",
        "df_test = df_test.drop(['Min_token_span', 'Has_correct_antonym'], axis=1)"
      ],
      "metadata": {
        "id": "lT3b25qNSyV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kaSPSFOsxr2"
      },
      "source": [
        "# Training and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKwjJOSwxXfA"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['Label']\n",
        "x_train = df_train.drop('Label', axis=1)\n",
        "y_val = df_dev['Label']\n",
        "x_val = df_dev.drop('Label', axis=1)\n",
        "y_test = df_test['Label']\n",
        "x_test = df_test.drop('Label', axis=1)\n",
        "\n",
        "classifier = lgbm.LGBMClassifier(n_estimators=100)\n",
        "\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "clsf_report1 = pd.DataFrame(classification_report(y_true = df_test['Label'], y_pred = predictions, output_dict = True)).transpose()\n",
        "confusion_matrix = pd.crosstab(df_test['Label'], predictions, rownames=['Actual'], colnames = ['Predicted'])\n",
        "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.suptitle('LGB'\n",
        ")\n",
        "plt.savefig('LGB.pdf')\n",
        "plt.show()\n",
        "\n",
        "feature_importances = classifier.feature_importances_ # Get feature importances\n",
        "importances = pd.Series(feature_importances, x_train.columns)\n",
        "fig, ax = plt.subplots()\n",
        "importances.plot.bar(ax=ax)\n",
        "ax.set_title(\"Feature importances\")\n",
        "plt.savefig('LGBM_feature importance.pdf')\n",
        "clsf_report1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-RMiCtT6MkU"
      },
      "source": [
        "# Training LGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-parameter tuning"
      ],
      "metadata": {
        "id": "xq0dTJ5i8Srv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HDNopx96N-o"
      },
      "outputs": [],
      "source": [
        "clf_params = {\n",
        "    'classifier__num_leaves':         sp_randint(7, 2095),\n",
        "    'classifier__reg_alpha':          [0, 1e-1, 1, 2, 5, 7, 10],\n",
        "    'classifier__reg_lambda':         [0, 1e-1, 1, 5, 10, 20],\n",
        "    'classifier__max_depth':          sp_randint(2, 50),\n",
        "    'classifier__objective':          ['cross_entropy', 'softmax']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-Swdb7r6iTL"
      },
      "outputs": [],
      "source": [
        "clf = Pipeline(\n",
        "    steps=[\n",
        "            ('classifier',  lgbm.LGBMClassifier())\n",
        "        ]\n",
        "    )\n",
        "\n",
        "rscv = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions = clf_params,\n",
        "    cv = 5, # 80% train 20% val\n",
        "    verbose = 1,\n",
        "    n_jobs = 2\n",
        ")\n",
        "\n",
        "rscv.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1slkGg-75D0"
      },
      "outputs": [],
      "source": [
        "classifier_LGB = rscv.best_estimator_\n",
        "print(\"Best parameters rscv LGB:\", rscv.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = classifier_LGB.predict(x_test)\n",
        "\n",
        "clsf_report1 = pd.DataFrame(classification_report(y_true = df_test['Label'], y_pred = predictions, output_dict = True)).transpose()\n",
        "confusion_matrix = pd.crosstab(df_test['Label'], predictions, rownames=['Actual'], colnames = ['Predicted'])\n",
        "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.suptitle('LGB')\n",
        "plt.show()\n",
        "\n",
        "feature_importances = classifier.feature_importances_ # Get feature importances\n",
        "importances = pd.Series(feature_importances, x_train.columns)\n",
        "fig, ax = plt.subplots()\n",
        "importances.plot.bar(ax=ax)\n",
        "ax.set_title(\"Feature importances\")\n",
        "\n",
        "clsf_report1"
      ],
      "metadata": {
        "id": "6xY-mOEZaLmu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LGBM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}